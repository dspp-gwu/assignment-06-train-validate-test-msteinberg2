{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Putting some rigor in traditional statistics\n",
    "\n",
    "Statisticians have long been aware of the problem of over-fitting. When a researcher runs a regression, they typically do so because they want to explore or test a hypothesis about how the world works beyond the data they have at hand.  They are trying to make a finding that *generalizes*. Overfitting is a term that is used to describe cases where a researcher develops a model that fits the data they have very well, but does not generalize to other data. \n",
    "\n",
    "Over-fitting can happen in many ways, and the disciplines of traditional statistics and machine learning have developed related methods for dealing with the problem. This week, we are looking at some of the methods that were developed in statistics to address the issue, methods that were further systematized in the field of machine learning. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The more data the better\n",
    "\n",
    "The first thing you need to know is that reliable findings -- findings that generalize -- typically require a lot of data.  The fewer data you have, the greater the risk of non-representation.  Further, the fewer data you have, the easier it is to explain all the statistical variation in the data with a few variables. There are a few rules of thumb about the size of your dataset, but one is that for any number of parameters (variables in your model), you should have at least somewhere between 10x and 50x that number in each of your subsets of data (train, validate, and test).  That typically means an N of at least 50x the number of parameters in your model. So if you have 10 parameters, you should have 500 or more rows of data. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Divide and conquer\n",
    "\n",
    "The second thing that good statisticians do is divide up their data into three data sets, typically (but not always) using a 60/20/20 split on smaller data sets.  (If you have hundreds of thousands or millions of rows, then you can devote smaller portions to validating and testing.)  \n",
    "\n",
    "The training set, which typically has 60% of the data, is safe for the statistician to explore using any techniques they like. Data mining is allowed and even encouraged.  Why? Becuase any model that is built on the training data will have to be validated on the second set.  And finally, once a model that has been trained and validated, it will then be tested against the third set. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Diving In \n",
    "\n",
    "In what follows, we'll grab some data and run it through the paces. Next week we will automate all of this and more, but for now, we'll do it by hand, so to speak, so you can see how much having it automated can improve your life. \n",
    "\n",
    "## A note of caution\n",
    "\n",
    "In looking at crime data, we'll want a motivation.  What are we going to *do* with the model.  Let's assume for now that we're working towards a version of PredPol -- the predictive polcing model that we've seen criticized in our readings.  Seeing how these models are developed is useful so that we can give a more sophisticated evaluation of them. Eventually, we'll definitely want to look at it as a time series, and that will give us plenty of data, but we're not quite ready to develop our own super-sophisticated model just yet.  For now, we'll just be running regressions on yearly aggregates from 2010, and that will limit our number of observatios.  Time series, and the problem of how to manage all the data associated with time will come later; the moral and policy problems associated with predicting crime are already up for discussion, but will beome more pointed as the weeks go on.\n",
    "\n",
    "## Another (more serious) note of caution\n",
    "\n",
    "As with many historic ills, crime is concentrated in historically disadvantaged communities subject to extended implicit and explicit discrimination.  It is impossible to have a responsible discussion about crime without also talking about the history of redlining, urban neglect by both the private and public sectors (including policing), and the role explicit and implicit bias in driving disproportionate portions of Black and Latino residents into jail.  But what does this mean for us when building a model predicting crime?  If we include race and ethnicity in our model, are we saying -- or letting others infer -- that there is something inherent in the residents those labels describe that is intrinsically more criminal?  Or is it ok to use race and ethnicity as stand-ins for historic discrimination and disadvantage?  There is no simple answer. For this exercise, I'm going to avoid using race and ethnicity as proxies, and try to find other measures of disadvantage.  This will cost me some predictive power, but in the long-run, I think it will make people who use the model less likely to make the kind of improper inferences that can drive bad policy.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "\r\n",
      "# All requested packages already installed.\r\n",
      "# packages in environment at /opt/conda:\r\n",
      "#\r\n",
      "geopandas                 0.3.0                    py36_0    conda-forge\r\n"
     ]
    }
   ],
   "source": [
    "# load our packages\n",
    "! conda install geopandas -qy\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm \n",
    "import statsmodels.formula.api as smf \n",
    "import matplotlib.pyplot as plt\n",
    "% matplotlib  inline\n",
    "import seaborn as sns; sns.set(color_codes=True)\n",
    "import urllib.request\n",
    "from shapely.geometry import Point\n",
    "\n",
    "\n",
    "# I've also made a little function for grabbing ACS data via the API. \n",
    "# The census data is great, but it's not the easiest to figure out.\n",
    "# If you want ACS data for your own project, feel free to poach this function.  :-)\n",
    "\n",
    "def get_acs_2010(varlist):\n",
    "    if len(varlist) > 50: \n",
    "        print('No more than 50 vars can be accessed via the API')\n",
    "        return\n",
    "    asv = ','.join(varlist)\n",
    "    base = 'https://api.census.gov/data/'\n",
    "    year = '2010/'\n",
    "    data_set = 'acs5'\n",
    "    var_list = '''?get=''' + asv \n",
    "    for_tract = '&for=tract:*'\n",
    "    in_state = '&in=state:011'\n",
    "    in_county = '&in=county:*'\n",
    "    api_key = '&key=8a94013b188270dc1caf589561d93fbed8091e0a'\n",
    "    url = base+year+data_set+var_list+for_tract+in_state+in_county+api_key\n",
    "    with urllib.request.urlopen(url) as f:\n",
    "         data = eval(f.read().decode('utf-8'))\n",
    "    acs_df = pd.DataFrame(data[1:],columns=data[0])\n",
    "    acs_df[varlist] = acs_df[varlist].astype(int)\n",
    "    acs_df['TRACT'] = acs_df['tract'].astype(str)\n",
    "    return acs_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What follows is the kind of data manipulation that is the vast majority of the work of a data scientist. It's not all that fun but we have to do it.  We live for models, but we spend most of our time rearranging data to feed into the models.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2017-10-03 04:23:24--  https://opendata.arcgis.com/datasets/6969dd63c5cb4d6aa32f15effb8311f3_8.geojson\r\n",
      "Resolving opendata.arcgis.com (opendata.arcgis.com)... 52.1.51.228, 54.174.84.28\r\n",
      "Connecting to opendata.arcgis.com (opendata.arcgis.com)|52.1.51.228|:443... connected.\r\n",
      "HTTP request sent, awaiting response... 200 OK\r\n",
      "Length: unspecified [application/json]\r\n",
      "Saving to: ‘census.geojson’\r\n",
      "\r\n",
      "\r",
      "census.geojson          [<=>                 ]       0  --.-KB/s               \r",
      "census.geojson          [ <=>                ]   1.71M  --.-KB/s    in 0.02s   \r\n",
      "\r\n",
      "2017-10-03 04:23:24 (104 MB/s) - ‘census.geojson’ saved [1794478]\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "# get our crimes data\n",
    "crimes = pd.read_csv('https://opendata.arcgis.com/datasets/fdacfbdda7654e06a161352247d3a2f0_34.csv')\n",
    "crimes = crimes[(crimes['OFFENSE']=='HOMICIDE') | \n",
    "                (crimes['OFFENSE']=='ASSAULT W/DANGEROUS WEAPON') |\n",
    "                (crimes['OFFENSE']=='SEX ABUSE')]\n",
    "\n",
    "# get some census geojson data\n",
    "! wget https://opendata.arcgis.com/datasets/6969dd63c5cb4d6aa32f15effb8311f3_8.geojson -O census.geojson\n",
    "census = gpd.read_file('census.geojson')\n",
    "\n",
    "# merge the crimes data and the census geojson data\n",
    "geometry = [Point(xy) for xy in zip(crimes.LONGITUDE.apply(float), crimes.LATITUDE.apply(float))]\n",
    "crs = {'init': 'epsg:4326'}\n",
    "points = gpd.GeoDataFrame(crimes, crs=crs, geometry=geometry)\n",
    "geo_crimes = gpd.sjoin(census, points, how='left', op='intersects')\n",
    "\n",
    "crime_percap = pd.DataFrame(geo_crimes.OBJECTID_left.value_counts()*100000/geo_crimes.P0010001)\n",
    "crime_percap.columns = ['crime_percap']\n",
    "tracts_crimes = census.merge(crime_percap, how=\"left\", left_on='OBJECTID', right_index=True)\n",
    "\n",
    "crime_count = pd.DataFrame(geo_crimes.OBJECTID_left.value_counts())\n",
    "crime_count.columns = ['crime_count']\n",
    "tracts_crimes = tracts_crimes.merge(crime_count, how=\"left\", left_on='OBJECTID', right_index=True)\n",
    "tracts_crimes['population_density'] = tracts_crimes['P0010001'] / tracts_crimes['SQ_MILES']\n",
    "\n",
    "# get some more census data using the census API and merge it with our other data\n",
    "#  I'll make a list of variables -- these are all basic age by gender variables\n",
    "varlist = [('B01001_'+str('%03d' % var)+'E') for var in range(1,50)] \n",
    "age_and_sex = get_acs_2010(varlist)\n",
    "\n",
    "varlist = [('B23009_'+str('%03d' % var)+'E') for var in range(1,44)] \n",
    "family = get_acs_2010(varlist)\n",
    "\n",
    "varlist = [('B17026_'+str('%03d' % var)+'E') for var in range(1,13)] \n",
    "poverty = get_acs_2010(varlist)\n",
    "\n",
    "\n",
    "# now I'll merge this data by tract. \n",
    "df = tracts_crimes.merge(age_and_sex, how=\"left\", on='TRACT', )\n",
    "df = df.merge(poverty, how=\"left\", on='TRACT', )\n",
    "df = df.merge(family, how=\"left\", on='TRACT', )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 01 - try loading some other census data.  \n",
    "\n",
    "Add any variables you want.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What did you choose to add to the data, and why? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splitting our data\n",
    "\n",
    "OK, so let's think about this -- we don't have a ton of observations, and we're about to split our data into even small subsamples.  A random spliting of 50 / 25 / 25 would give us roughly 90 observations to train on, 45 for validation, and 45 for testing.  That's really not much. The point of dividing our data is to give us a little more confidence that a result actually will generalize to the real world.  We can probably only afford to have a few parameters in our final model.  So let's do what good statisticians did for a while.  Let's hide away some data, build some promising models, validate them, and then see how they perform on our test data.  \n",
    "\n",
    "If we had a huge data set, we could do this via random sampling.  But we have relatively little data, and we want to make sure that we get a roughly representative selection of census tracts across the crime-spectrum in each subset. There are fancy ways to do this, but the easiest way is just to sort the data by crime rate, then break it into portions.  So that's what we'll start by doing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# first, let's make sure we get data that is sampled across the range of crimes per capita \n",
    "# this will ensure that we don't accidentally select \n",
    "\n",
    "df = df.sort_values(by='crime_percap').reset_index()\n",
    "train_mask = list(range(0,180,4))\n",
    "train_mask = train_mask + list(range(1,180,4))\n",
    "train_mask.sort()\n",
    "validate_mask = list(range(2,180,4))\n",
    "test_mask = list(range(3,180,4))\n",
    "\n",
    "train = df.loc[train_mask]\n",
    "validate = df.loc[validate_mask]\n",
    "test = df.loc[test_mask]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our data sets, we can go to town on the training data.  Let's say we want to model criminal incidence.  In the next few weeks, we'll do this using a lot more data spread across time.  But for now, let's just get the basic idea with the data we have at hand.  \n",
    "\n",
    "One way to figure this sort of thing out is to look at simple correlations in the data we have.  And the easiest way to do that, is to just have a look at what correlates most highly with crime rates.  \n",
    "\n",
    "# exercise 02 - Examine your correlates \n",
    "\n",
    "Type in the following: \n",
    "\n",
    "```train.corr()['crime_count'].sort_values()```\n",
    "\n",
    "What do you see? Anything you think should go into your model? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One thing that I can see is that income is very strongly negatively correlated with crime. But if we look deeply at the variables (I won't do that here -- feel free to export to a CSV of the variable descriptions and look through them yourself), there are also a bunch of demographic variables like age, gender, and some housing-related variables like the number of vacant units.  Race is also highly correlated with crime, but my goal right now -- for reasons discussed above -- is to develop a model that doesn't rely on race or ethnicity. \n",
    "\n",
    "Variabls negatively correlated with crime are income and older populations: \n",
    "- median income  FAGI_MEDIAN_2010\n",
    "- men over 60 B01001_18E - 25\n",
    "- women over 60 B01001_42E - 49\n",
    "- married couples, B23009_024E + B23009_003E\n",
    "\n",
    "Variables positively correlated with crime:\n",
    "- vacant properties H0010003\n",
    "- population under 18 female: B01001_027E B01001_028E B01001_029E B01001_030E male: B01001_003E B01001_004E B01001_005E B01001_006E\n",
    "- single mothers B23009_018E & B23009_019E \n",
    "- single females, no kids B23009_039E 40 & 42 \n",
    "- high poverty B17026_002E B17026_003E B17026_004E\n",
    "\n",
    "If you know anything about crime and the criminal justice system, then you know that both are hard on family life.  So it should be unsurprising that crime (and thus involvement in the criminal justice system) are highly correlated with absent fathers, and kids living with non-parents. There is thus a powerful two-way relationship between household structure and crime that mirrors historical disadvantage and mass-incarceration. Here, again, we have to be very careful.  There is a story about people living in low-income high-crime neighborhoods that pays little attention to how poverty, crime, and a dysfunctional criminal justice system can, over decades, pull families and communities apart. Thankfully, a lot of research has turned that story around, and most policy makers are interested in working on reducing improper policing and incarceration rates.  Still, it is worth noting that a model that a model that isn't carefully described could feed into a story that we don't intend to tell. \n",
    "\n",
    "And here's a question -- why do you think older men and women are correlated with lower crime rates?  I can think of a few theories -- longevity is another indicator of wealth and community wellbeing; or maybe they pay attention to what is going on in their neighborhood and reign in bad behavior -- and each suggests a different way of thinking about crime and aging. The question I have is whether it matters to the goal of building a (not very sophisticated) PredPol model.  \n",
    "\n",
    "With those cautions in mind, let's construct some of these variables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['income_log'] = df['FAGI_MEDIAN_2010'].apply(np.log)\n",
    "df['crimepc_log'] = df['crime_percap'].apply(np.log)\n",
    "df['crime_log'] = df['crime_count'].apply(np.log)\n",
    "\n",
    "varlist = [('B01001_'+str('%03d' % var)+'E') for var in range(18,26)] \n",
    "df['senior_men'] = df[varlist].astype(int).sum(axis=1 )\n",
    "\n",
    "varlist = [('B01001_'+str('%03d' % var)+'E') for var in range(42,50)] \n",
    "df['senior_women'] = df[varlist].astype(int).sum(axis=1 )\n",
    "\n",
    "varlist = ['B23009_024E','B23009_003E'] \n",
    "df['married_couples'] = df[varlist].astype(int).sum(axis=1 )\n",
    "\n",
    "varlist = ['H0010003'] \n",
    "df['vacants'] = df[varlist].astype(int).sum(axis=1 )\n",
    "\n",
    "varlist = ['B01001_027E','B01001_028E','B01001_029E','B01001_030E','B01001_003E','B01001_004E','B01001_005E','B01001_006E'] \n",
    "df['youth'] = df[varlist].astype(int).sum(axis=1 )\n",
    "\n",
    "varlist = ['B23009_018E','B23009_019E'] # ,'B23009_039E','B23009_040E','B23009_041E','B23009_042E' \n",
    "df['solo_women'] = df[varlist].astype(int).sum(axis=1 )\n",
    "\n",
    "varlist = ['B17026_002E','B17026_003E','B17026_004E'] \n",
    "df['high_poverty'] = df[varlist].astype(int).sum(axis=1 )\n",
    "\n",
    "train = df.loc[train_mask]\n",
    "validate = df.loc[validate_mask]\n",
    "test = df.loc[test_mask]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploration with data mining. \n",
    "\n",
    "Last week, data mining was the enemy.  It gave us absurd results and models. But this week, we know that we can guard against overfitting by training and validating, and then testing.  \n",
    "\n",
    "We aren't in the world of real training and testing yet, but let's try to do a little automation.  At this point we have a solid method.  So we can go to town (and you'll learn even better ways to do this soon) in testing out models that we might want to deploy.  Let's return to our simple data mining function (the technical term for it is a regression with \"forward selection\") from last week.  It just runs through all the additive combinations of the variables to see if any single variable increases the r-squared.  There are still dangers here, but we can afford to be a bit reckless because we'll be disciplined by our validation and test steps. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.280106379744\n",
      "0.40833860162\n",
      "0.505582652232\n",
      "0.553030598849\n",
      "0.570613300016\n",
      "0.636075443933\n",
      "0.66527824674\n",
      "0.723243191606\n",
      "0.752745001322\n",
      "crime_count ~ youth + income_log + married_couples + vacants + senior_men + FAGI_MEDIAN_2010 + solo_women + senior_women + population_density + 1\n",
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:            crime_count   R-squared:                       0.778\n",
      "Model:                            OLS   Adj. R-squared:                  0.753\n",
      "Method:                 Least Squares   F-statistic:                     31.11\n",
      "Date:                Tue, 03 Oct 2017   Prob (F-statistic):           1.34e-22\n",
      "Time:                        04:23:26   Log-Likelihood:                -33.537\n",
      "No. Observations:                  90   AIC:                             87.07\n",
      "Df Residuals:                      80   BIC:                             112.1\n",
      "Df Model:                           9                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "======================================================================================\n",
      "                         coef    std err          t      P>|t|      [0.025      0.975]\n",
      "--------------------------------------------------------------------------------------\n",
      "Intercept             31.7728      2.905     10.937      0.000      25.992      37.554\n",
      "youth                 -0.0047      0.000    -10.430      0.000      -0.006      -0.004\n",
      "income_log            -3.1514      0.313    -10.062      0.000      -3.775      -2.528\n",
      "married_couples        0.0055      0.001      9.242      0.000       0.004       0.007\n",
      "vacants               -0.0016      0.001     -2.502      0.014      -0.003      -0.000\n",
      "senior_men            -0.0102      0.001     -7.628      0.000      -0.013      -0.008\n",
      "FAGI_MEDIAN_2010    5.965e-05   7.59e-06      7.861      0.000    4.46e-05    7.48e-05\n",
      "solo_women             0.0153      0.003      5.364      0.000       0.010       0.021\n",
      "senior_women           0.0047      0.001      5.517      0.000       0.003       0.006\n",
      "population_density  1.992e-05    6.1e-06      3.266      0.002    7.78e-06    3.21e-05\n",
      "==============================================================================\n",
      "Omnibus:                       22.473   Durbin-Watson:                   0.935\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):               43.549\n",
      "Skew:                           0.937   Prob(JB):                     3.49e-10\n",
      "Kurtosis:                       5.846   Cond. No.                     6.02e+06\n",
      "==============================================================================\n",
      "\n",
      "Warnings:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "[2] The condition number is large, 6.02e+06. This might indicate that there are\n",
      "strong multicollinearity or other numerical problems.\n"
     ]
    }
   ],
   "source": [
    "def data_mining(data, dv):\n",
    "    remaining = set(data.columns)\n",
    "    remaining.remove(dv)\n",
    "    selected = []\n",
    "    current_score, best_new_score = 0.0, 0.0\n",
    "    while remaining and current_score == best_new_score:\n",
    "        scores_with_candidates = []\n",
    "        for candidate in remaining:\n",
    "            formula = \"{} ~ {} + 1\".format(dv, ' + '.join(selected + [candidate]))\n",
    "            score = smf.ols(formula, data).fit().rsquared_adj\n",
    "            scores_with_candidates.append((score, candidate))\n",
    "        scores_with_candidates.sort()\n",
    "        best_new_score, best_candidate = scores_with_candidates.pop()\n",
    "        if current_score < best_new_score:\n",
    "            remaining.remove(best_candidate)\n",
    "            selected.append(best_candidate)\n",
    "            current_score = best_new_score\n",
    "            print(current_score)\n",
    "            if score  == 1: \n",
    "                print('model overfitted: ' + selected)\n",
    "    formula = \"{} ~ {} + 1\".format(dv, ' + '.join(selected))\n",
    "    print(formula)\n",
    "    model = smf.ols(formula, data).fit()\n",
    "    return model\n",
    "\n",
    "myvars = ['crime_count', 'FAGI_MEDIAN_2010', 'income_log', 'high_poverty', 'vacants', 'population_density',  \n",
    "          'senior_men', 'senior_women' , 'married_couples', 'youth', 'solo_women']\n",
    "df_train = pd.DataFrame(train[myvars])\n",
    "mining = data_mining(df_train,'crime_count')\n",
    "mining.summary()\n",
    "\n",
    "print(mining.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reviewing the exploratory results\n",
    "\n",
    "What our forward selection model is telling us is that if we remove any of these parameters, our fit to the data will decrease.  In fancier versions of data mining, we could have it tell us how to drop to successively smaller sets of parameters while preserving the greatest fit.   But we're going to move way beyond this kind of methodology soon, but the approach contains some ideas that we'll be using later. We try to build a model that fits the training data and see if it is robust enough to survive exposure to new data.  \n",
    "\n",
    "There are a few things that tell me that this is overfitted.  First, if we have four measures of the same thing (income, wealth, poverty), we're going to have a problem in that the specific combination of them is probably highly collinear and likely to cause overfitting. In a more advanced setting, we'd try some version of what is called \"dimensionality reduction\" that reduces the number of parameters using \"principal compenents\" or \"factors\" or \"clusters\".  All of them find some way of combining (and thus reducing the number of) the parameters involved.  \n",
    "\n",
    "For now, let's reduce it the old fashioned way: toss some out. The old-fashioned way involves using informed hunches about the data, often built by long exposure to the data and other studies.  \n",
    "\n",
    "There is a research indicating that concentrated poverty is a serious problem, and concentrated should be even less correlated with (reported) criminality.  For that reason, we may want to interact income and population density.    \n",
    "\n",
    "There is also research about the way vacancies can lead to crime -- but moreso in low-income neighborhoods.  This differential effect (more powerful in low-income tracts than high-income tracts) makes it another candidate for interaction with income.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:            crime_count   R-squared:                       0.487\n",
      "Model:                            OLS   Adj. R-squared:                  0.456\n",
      "Method:                 Least Squares   F-statistic:                     15.93\n",
      "Date:                Tue, 03 Oct 2017   Prob (F-statistic):           5.17e-11\n",
      "Time:                        04:23:26   Log-Likelihood:                -71.206\n",
      "No. Observations:                  90   AIC:                             154.4\n",
      "Df Residuals:                      84   BIC:                             169.4\n",
      "Df Model:                           5                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "=================================================================================================\n",
      "                                    coef    std err          t      P>|t|      [0.025      0.975]\n",
      "-------------------------------------------------------------------------------------------------\n",
      "Intercept                       -12.4136      5.352     -2.319      0.023     -23.057      -1.770\n",
      "income_log                        1.2066      0.475      2.541      0.013       0.262       2.151\n",
      "population_density                0.0013      0.000      4.388      0.000       0.001       0.002\n",
      "vacants                           0.0714      0.032      2.258      0.027       0.009       0.134\n",
      "income_log:population_density    -0.0001   2.66e-05     -4.332      0.000      -0.000   -6.25e-05\n",
      "income_log:vacants               -0.0063      0.003     -2.226      0.029      -0.012      -0.001\n",
      "==============================================================================\n",
      "Omnibus:                        4.029   Durbin-Watson:                   0.469\n",
      "Prob(Omnibus):                  0.133   Jarque-Bera (JB):                4.008\n",
      "Skew:                           0.482   Prob(JB):                        0.135\n",
      "Kurtosis:                       2.626   Cond. No.                     1.48e+07\n",
      "==============================================================================\n",
      "\n",
      "Warnings:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "[2] The condition number is large, 1.48e+07. This might indicate that there are\n",
      "strong multicollinearity or other numerical problems.\n"
     ]
    }
   ],
   "source": [
    "# testing for interaction effects\n",
    "model = 'crime_count ~  income_log * (population_density + vacants)  '\n",
    "results = smf.ols(model, data=train).fit()\n",
    "print(results.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sure enough there are significant interaction effects, and their inclusion increases the r squared and the (lower) AIC & BIC. \n",
    "\n",
    "# Exercise 03 - Try playing with different models.  \n",
    "\n",
    "Add and subtract parameters from your model. Try interacting (putting a * between) some parameters. What happens to teh r-squared?  What happens to the AIC & BIC? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 04 - validate your most promising models\n",
    "\n",
    "Once you have two or three models you want to try out, apply the same method of fitting your model, but this time to the validate dataset.  How does each compare?  Is the adjusted R-squared larger or smaller?  Are the AIC & BIC larger or smaller? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# this seems both painfully manual and kinda random\n",
    "\n",
    "For a long time, this was what it meant to do simple statistical research.  Develop a bunch of models that seems promising, then throw them at a validation data set.  Iterate until you have something you think is relatively robust, then test the hypotheses you've developed against test data.  \n",
    "\n",
    "Much of this will become automated pretty soon.  Next week in fact.  For now, let's just jump to the conclusion: let's test the model we developed. Since we have our three data sets, let's just change the data from validate to test, and we're good to go. \n",
    "\n",
    "# Exercise 05 - fitting to our test data\n",
    "\n",
    "using the same smf.ols model from above, fit it to the test data, adn print a summary of the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do you notice in the results?  Were they what you expected?  How would you judge the quality of our model?  Did we succeed in making a relatively robust model? \n",
    "\n",
    "# Exercise 06 - your data, your model\n",
    "\n",
    "Now try doing all of this with some of your data. Keep it simple for now.  What are you including in your model?  How do variations on your model compare with one another? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
